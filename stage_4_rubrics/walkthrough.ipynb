{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 4: Rubric-Based Evaluation ‚Äî Multi-Dimensional Quality Scoring\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ What You'll Learn\n",
        "\n",
        "By the end of this walkthrough, you'll understand:\n",
        "\n",
        "1. **What rubrics are** and why pass/fail isn't enough\n",
        "2. **The four quality dimensions** and how to weight them\n",
        "3. **How LLM-as-judge scoring works** behind the scenes\n",
        "4. **How to set quality thresholds** to prevent regressions\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Understanding Rubric-Based Evaluation\n",
        "\n",
        "### The Problem with Pass/Fail\n",
        "\n",
        "Previous stages tell you **if** something works. But:\n",
        "- A response can be \"correct\" but poorly written\n",
        "- Quality can gradually degrade before hard failures\n",
        "- Different use cases prioritize different qualities\n",
        "\n",
        "### What Are Rubrics?\n",
        "\n",
        "**Rubrics score responses across multiple dimensions**, giving you nuanced quality insights:\n",
        "\n",
        "| Dimension | What It Measures | Example |\n",
        "|-----------|-----------------|---------|\n",
        "| **Relevance** (0-5) | Does it answer the question? | Off-topic = 1, Perfect = 5 |\n",
        "| **Accuracy** (0-5) | Is the information correct? | Errors = 1, Verified = 5 |\n",
        "| **Completeness** (0-5) | Does it cover everything? | Partial = 3, Full = 5 |\n",
        "| **Clarity** (0-5) | Is it well-organized? | Confusing = 1, Clear = 5 |\n",
        "\n",
        "### Why Rubrics Matter\n",
        "\n",
        "```\n",
        "Score Trend Over Time:\n",
        "                        ‚ö†Ô∏è Quality degrading!\n",
        "  5.0 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "      ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
        "  4.0 ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
        "      ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
        "  3.0 ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚Üê Would pass binary tests\n",
        "      ‚îÇ                                       but rubric shows decline\n",
        "  2.0 ‚îÇ\n",
        "      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫\n",
        "         v1.0   v1.1   v1.2   v1.3   time\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è Where Rubrics Fit: The Eval Maturity Model\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    EVAL FRAMEWORK MATURITY                      ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                 ‚îÇ\n",
        "‚îÇ  Stage 5: Experiments      ‚Üê Compare configurations            ‚îÇ\n",
        "‚îÇ      ‚ñ≤                                                          ‚îÇ\n",
        "‚îÇ  ‚òÖ Stage 4: RUBRICS ‚òÖ      ‚Üê Multi-dimensional scoring (HERE)  ‚îÇ\n",
        "‚îÇ      ‚ñ≤                                                          ‚îÇ\n",
        "‚îÇ  Stage 3: Replay Harnesses ‚Üê Reproducibility (completed)       ‚îÇ\n",
        "‚îÇ      ‚ñ≤                                                          ‚îÇ\n",
        "‚îÇ  Stage 2: Labeled Scenarios‚Üê Coverage mapping (completed)      ‚îÇ\n",
        "‚îÇ      ‚ñ≤                                                          ‚îÇ\n",
        "‚îÇ  Stage 1: Golden Sets      ‚Üê Baseline correctness (completed)  ‚îÇ\n",
        "‚îÇ                                                                 ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "Rubrics add **gradual quality tracking** on top of the pass/fail foundations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Behind the Scenes: How Rubric Scoring Works\n",
        "\n",
        "Rubric scoring uses an **LLM-as-judge** pattern:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    RUBRIC SCORING FLOW                                ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                       ‚îÇ\n",
        "‚îÇ  INPUT:                                                               ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ  ‚îÇ Query:    \"What is our PTO policy?\"                         ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ Response: \"Employees receive 15 days of PTO annually...\"    ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ Sources:  [\"pto_policy.md\"]                                 ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îÇ                            ‚îÇ                                          ‚îÇ\n",
        "‚îÇ                            ‚ñº                                          ‚îÇ\n",
        "‚îÇ  LLM JUDGE (gpt-4o-mini):                                            ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ  ‚îÇ \"Score this response on each dimension (0-5):\"              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ                                                              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ - Relevance: Does it address the PTO question?              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ - Accuracy: Are the facts correct per the source?           ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ - Completeness: Does it cover all PTO details?              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ - Clarity: Is it well-organized?                            ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îÇ                            ‚îÇ                                          ‚îÇ\n",
        "‚îÇ                            ‚ñº                                          ‚îÇ\n",
        "‚îÇ  OUTPUT:                                                              ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ  ‚îÇ {                                                           ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   \"relevance\": {\"score\": 5, \"reason\": \"Directly answers\"},  ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   \"accuracy\": {\"score\": 4, \"reason\": \"Correct, minor...\"},  ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   \"completeness\": {\"score\": 4, \"reason\": \"Missing...\"},     ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   \"clarity\": {\"score\": 5, \"reason\": \"Well structured\"}      ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ }                                                           ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îÇ                            ‚îÇ                                          ‚îÇ\n",
        "‚îÇ                            ‚ñº                                          ‚îÇ\n",
        "‚îÇ  WEIGHTED SCORE:                                                      ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ  ‚îÇ relevance(5) √ó 0.25 + accuracy(4) √ó 0.35 +                  ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ completeness(4) √ó 0.25 + clarity(5) √ó 0.15 = 4.40           ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ                                                              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ Quality Level: \"Good\" (3.5-4.4 range)                       ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îÇ                                                                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**Key insight:** Weights can be customized per category (e.g., accuracy matters more for compliance questions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "import yaml\n",
        "from scorer import RubricScorer, print_result\n",
        "from evaluator import run_rubric_evaluation, print_summary, load_rubric_test_cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíª Hands-On: Exploring Rubric Structure\n",
        "\n",
        "Let's examine how rubrics are defined in YAML. Notice how each dimension has:\n",
        "- A description of what it measures\n",
        "- Specific criteria for each score level (0-5)\n",
        "- A weight for the final calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and explore the rubrics\n",
        "with open(\"rubrics.yaml\") as f:\n",
        "    rubrics = yaml.safe_load(f)\n",
        "\n",
        "print(\"Evaluation Dimensions:\")\n",
        "print(\"=\" * 50)\n",
        "for dim_name, dim_info in rubrics[\"dimensions\"].items():\n",
        "    print(f\"\\n{dim_info['name']} (weight: {dim_info['weight']:.0%})\")\n",
        "    print(f\"  {dim_info['description']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the scoring criteria for one dimension\n",
        "print(\"Accuracy Scoring Criteria:\")\n",
        "print(\"=\" * 50)\n",
        "for score, desc in rubrics[\"dimensions\"][\"accuracy\"][\"criteria\"].items():\n",
        "    print(f\"  {score}: {desc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Scoring a Single Response\n",
        "\n",
        "Let's run a query through the agent and see how the rubric scorer evaluates it. Watch for:\n",
        "- Individual dimension scores\n",
        "- Justifications from the judge\n",
        "- The weighted overall score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the agent from local config\n",
        "from rubric_config import ask_acme\n",
        "\n",
        "# Run a query\n",
        "query = \"What is our remote work policy?\"\n",
        "response = ask_acme(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Response:\\n{response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score the response\n",
        "scorer = RubricScorer()\n",
        "result = scorer.score(\n",
        "    query=query,\n",
        "    response=response,\n",
        "    sources=[\"remote_work_policy.md\"],\n",
        "    category=\"policy\"\n",
        ")\n",
        "\n",
        "print_result(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öñÔ∏è Category-Specific Weights\n",
        "\n",
        "**This is a powerful feature**: Different query types should prioritize different dimensions.\n",
        "\n",
        "| Category | Higher Weight For | Why |\n",
        "|----------|------------------|-----|\n",
        "| **Policy** | Accuracy | Wrong policy info = compliance risk |\n",
        "| **Metrics** | Completeness | Missing numbers = incomplete picture |\n",
        "| **Engineering** | Clarity | Devs need clear instructions |\n",
        "\n",
        "Let's see how weights differ by category:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Default Weights vs Category-Specific Weights\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "default_weights = scorer.get_weights()\n",
        "print(f\"\\nDefault:\")\n",
        "for dim, weight in default_weights.items():\n",
        "    print(f\"  {dim}: {weight:.0%}\")\n",
        "\n",
        "for category in [\"policy\", \"metrics\", \"engineering\"]:\n",
        "    weights = scorer.get_weights(category)\n",
        "    print(f\"\\n{category.capitalize()}:\")\n",
        "    for dim, weight in weights.items():\n",
        "        diff = weight - default_weights[dim]\n",
        "        indicator = \"‚Üë\" if diff > 0 else \"‚Üì\" if diff < 0 else \" \"\n",
        "        print(f\"  {dim}: {weight:.0%} {indicator}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèÉ Running a Full Evaluation Suite\n",
        "\n",
        "Let's run the complete rubric evaluation across multiple test cases. The summary will show:\n",
        "- Quality distribution (Excellent/Good/Acceptable/Poor/Critical)\n",
        "- Dimension averages (which dimensions are weakest?)\n",
        "- Lowest scoring cases (where to focus improvement?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test cases\n",
        "test_cases = load_rubric_test_cases(\"rubrics.yaml\")\n",
        "\n",
        "print(f\"Found {len(test_cases)} test cases:\")\n",
        "for case in test_cases[:5]:\n",
        "    print(f\"  [{case['id']}] {case['query'][:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on a subset (for speed)\n",
        "results, summary = run_rubric_evaluation(test_cases[:3], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View summary\n",
        "print_summary(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üö¶ Quality Gates and Thresholds\n",
        "\n",
        "**Quality gates** prevent regressions by failing builds when scores drop below thresholds.\n",
        "\n",
        "| Score | Quality Level | Action |\n",
        "|-------|--------------|--------|\n",
        "| 4.5-5.0 | Excellent | Ship it! |\n",
        "| 3.5-4.4 | Good | Minor improvements optional |\n",
        "| 2.5-3.4 | Acceptable | Review before shipping |\n",
        "| 1.5-2.4 | Poor | Must improve |\n",
        "| 0-1.4 | Critical | Block deployment |\n",
        "\n",
        "Let's implement quality gates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define minimum acceptable scores\n",
        "QUALITY_THRESHOLDS = {\n",
        "    \"overall\": 3.5,\n",
        "    \"accuracy\": 4.0,  # Accuracy is critical\n",
        "    \"relevance\": 3.5,\n",
        "    \"completeness\": 3.0,\n",
        "    \"clarity\": 3.0\n",
        "}\n",
        "\n",
        "def check_quality_gates(results):\n",
        "    \"\"\"Check if results meet quality thresholds.\"\"\"\n",
        "    failures = []\n",
        "    \n",
        "    # Check overall average\n",
        "    avg_overall = sum(r.overall_score for r in results) / len(results)\n",
        "    if avg_overall < QUALITY_THRESHOLDS[\"overall\"]:\n",
        "        failures.append(f\"Overall avg {avg_overall:.2f} < {QUALITY_THRESHOLDS['overall']}\")\n",
        "    \n",
        "    # Check dimension averages\n",
        "    for dim in [\"accuracy\", \"relevance\", \"completeness\", \"clarity\"]:\n",
        "        scores = [s.score for r in results for s in r.scores if s.dimension == dim]\n",
        "        avg = sum(scores) / len(scores)\n",
        "        if avg < QUALITY_THRESHOLDS[dim]:\n",
        "            failures.append(f\"{dim.capitalize()} avg {avg:.2f} < {QUALITY_THRESHOLDS[dim]}\")\n",
        "    \n",
        "    return failures\n",
        "\n",
        "# Check our results\n",
        "failures = check_quality_gates(results)\n",
        "if failures:\n",
        "    print(\"Quality Gate Failures:\")\n",
        "    for f in failures:\n",
        "        print(f\"  - {f}\")\n",
        "else:\n",
        "    print(\"All quality gates passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Try Your Own Query\n",
        "\n",
        "Test any query and see how it scores across all dimensions. Experiment with different:\n",
        "- Query types (policy vs metrics)\n",
        "- Categories (affects weights)\n",
        "- Complexities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change this to test different queries\n",
        "test_query = \"How do I request time off?\"\n",
        "test_category = \"policy\"  # optional: policy, metrics, engineering, cross_tool\n",
        "\n",
        "# Get response\n",
        "response = ask_acme(test_query)\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"Response: {response}\\n\")\n",
        "\n",
        "# Score it\n",
        "result = scorer.score(\n",
        "    query=test_query,\n",
        "    response=response,\n",
        "    category=test_category\n",
        ")\n",
        "print_result(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Key Takeaways\n",
        "\n",
        "1. **Rubrics catch gradual decline** ‚Äî Quality scores reveal degradation before hard failures\n",
        "2. **Four dimensions** ‚Äî Relevance, Accuracy, Completeness, Clarity\n",
        "3. **Weighted scoring** ‚Äî Customize weights per category (accuracy > clarity for compliance)\n",
        "4. **LLM-as-judge** ‚Äî Use a model to evaluate another model's output\n",
        "5. **Quality gates** ‚Äî Set thresholds to prevent regressions in CI\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Interpreting Rubric Results\n",
        "\n",
        "| What You See | What It Means | What To Do |\n",
        "|-------------|---------------|------------|\n",
        "| Low Relevance | Agent misunderstanding queries | Improve routing or system prompt |\n",
        "| Low Accuracy | Facts wrong or hallucinated | Better retrieval or source grounding |\n",
        "| Low Completeness | Missing parts of answer | Improve prompting for thoroughness |\n",
        "| Low Clarity | Confusing responses | Format instructions in system prompt |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è≠Ô∏è What's Next?\n",
        "\n",
        "Rubrics tell you quality scores, but how do you improve them? You need to:\n",
        "- Compare different configurations (models, prompts, temperatures)\n",
        "- Make data-driven decisions about what works best\n",
        "- Track improvements over time\n",
        "\n",
        "**Stage 5: Experiments** lets you systematically compare agent variants:\n",
        "\n",
        "```bash\n",
        "cd ../stage_5_experiments\n",
        "uv run jupyter notebook walkthrough.ipynb\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
