{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 4: Rubric-Based Evaluation\n",
        "\n",
        "This notebook walks through **multi-dimensional scoring** of AI responses using rubrics.\n",
        "\n",
        "Unlike pass/fail tests, rubrics grade responses across multiple quality dimensions:\n",
        "- **Relevance**: Does it answer the question?\n",
        "- **Accuracy**: Is the information correct?\n",
        "- **Completeness**: Does it cover everything needed?\n",
        "- **Clarity**: Is it well-organized and readable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "import yaml\n",
        "from scorer import RubricScorer, print_result\n",
        "from evaluator import run_rubric_evaluation, print_summary, load_rubric_test_cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Rubrics\n",
        "\n",
        "Let's examine how rubrics are structured:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and explore the rubrics\n",
        "with open(\"rubrics.yaml\") as f:\n",
        "    rubrics = yaml.safe_load(f)\n",
        "\n",
        "print(\"Evaluation Dimensions:\")\n",
        "print(\"=\" * 50)\n",
        "for dim_name, dim_info in rubrics[\"dimensions\"].items():\n",
        "    print(f\"\\n{dim_info['name']} (weight: {dim_info['weight']:.0%})\")\n",
        "    print(f\"  {dim_info['description']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the scoring criteria for one dimension\n",
        "print(\"Accuracy Scoring Criteria:\")\n",
        "print(\"=\" * 50)\n",
        "for score, desc in rubrics[\"dimensions\"][\"accuracy\"][\"criteria\"].items():\n",
        "    print(f\"  {score}: {desc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scoring a Single Response\n",
        "\n",
        "Let's run a query through the agent and score it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the agent from local config\n",
        "from rubric_config import ask_acme\n",
        "\n",
        "# Run a query\n",
        "query = \"What is our remote work policy?\"\n",
        "response = ask_acme(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Response:\\n{response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score the response\n",
        "scorer = RubricScorer()\n",
        "result = scorer.score(\n",
        "    query=query,\n",
        "    response=response,\n",
        "    sources=[\"remote_work_policy.md\"],\n",
        "    category=\"policy\"\n",
        ")\n",
        "\n",
        "print_result(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Category-Specific Weights\n",
        "\n",
        "Different query types may prioritize different dimensions. For example:\n",
        "- **Policy questions**: Accuracy is critical\n",
        "- **Metrics questions**: Completeness matters more\n",
        "\n",
        "Let's compare the weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Default Weights vs Category-Specific Weights\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "default_weights = scorer.get_weights()\n",
        "print(f\"\\nDefault:\")\n",
        "for dim, weight in default_weights.items():\n",
        "    print(f\"  {dim}: {weight:.0%}\")\n",
        "\n",
        "for category in [\"policy\", \"metrics\", \"engineering\"]:\n",
        "    weights = scorer.get_weights(category)\n",
        "    print(f\"\\n{category.capitalize()}:\")\n",
        "    for dim, weight in weights.items():\n",
        "        diff = weight - default_weights[dim]\n",
        "        indicator = \"↑\" if diff > 0 else \"↓\" if diff < 0 else \" \"\n",
        "        print(f\"  {dim}: {weight:.0%} {indicator}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running a Full Evaluation\n",
        "\n",
        "Let's run the complete rubric evaluation suite:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test cases\n",
        "test_cases = load_rubric_test_cases(\"rubrics.yaml\")\n",
        "\n",
        "print(f\"Found {len(test_cases)} test cases:\")\n",
        "for case in test_cases[:5]:\n",
        "    print(f\"  [{case['id']}] {case['query'][:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on a subset (for speed)\n",
        "results, summary = run_rubric_evaluation(test_cases[:3], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View summary\n",
        "print_summary(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quality Thresholds\n",
        "\n",
        "Set thresholds to catch quality regressions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define minimum acceptable scores\n",
        "QUALITY_THRESHOLDS = {\n",
        "    \"overall\": 3.5,\n",
        "    \"accuracy\": 4.0,  # Accuracy is critical\n",
        "    \"relevance\": 3.5,\n",
        "    \"completeness\": 3.0,\n",
        "    \"clarity\": 3.0\n",
        "}\n",
        "\n",
        "def check_quality_gates(results):\n",
        "    \"\"\"Check if results meet quality thresholds.\"\"\"\n",
        "    failures = []\n",
        "    \n",
        "    # Check overall average\n",
        "    avg_overall = sum(r.overall_score for r in results) / len(results)\n",
        "    if avg_overall < QUALITY_THRESHOLDS[\"overall\"]:\n",
        "        failures.append(f\"Overall avg {avg_overall:.2f} < {QUALITY_THRESHOLDS['overall']}\")\n",
        "    \n",
        "    # Check dimension averages\n",
        "    for dim in [\"accuracy\", \"relevance\", \"completeness\", \"clarity\"]:\n",
        "        scores = [s.score for r in results for s in r.scores if s.dimension == dim]\n",
        "        avg = sum(scores) / len(scores)\n",
        "        if avg < QUALITY_THRESHOLDS[dim]:\n",
        "            failures.append(f\"{dim.capitalize()} avg {avg:.2f} < {QUALITY_THRESHOLDS[dim]}\")\n",
        "    \n",
        "    return failures\n",
        "\n",
        "# Check our results\n",
        "failures = check_quality_gates(results)\n",
        "if failures:\n",
        "    print(\"Quality Gate Failures:\")\n",
        "    for f in failures:\n",
        "        print(f\"  - {f}\")\n",
        "else:\n",
        "    print(\"All quality gates passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try Your Own Query\n",
        "\n",
        "Test a query and see how it scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change this to test different queries\n",
        "test_query = \"How do I request time off?\"\n",
        "test_category = \"policy\"  # optional: policy, metrics, engineering, cross_tool\n",
        "\n",
        "# Get response\n",
        "response = ask_acme(test_query)\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"Response: {response}\\n\")\n",
        "\n",
        "# Score it\n",
        "result = scorer.score(\n",
        "    query=test_query,\n",
        "    response=response,\n",
        "    category=test_category\n",
        ")\n",
        "print_result(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Rubrics provide nuance**: Quality scores reveal gradual degradation before hard failures\n",
        "2. **Multiple dimensions**: Different aspects of quality can be tracked independently\n",
        "3. **Category weights**: Customize what matters for different query types\n",
        "4. **Integration**: Rubrics layer on top of golden sets and scenarios\n",
        "5. **Quality gates**: Set thresholds to prevent quality regressions\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Calibrate rubric weights for your specific use case\n",
        "- Set up automated quality tracking over time\n",
        "- Move to CI integration for continuous evaluation"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
