{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Golden Sets Walkthrough\n",
        "\n",
        "This notebook walks through the golden set evaluation process - your first line of defense for AI quality.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. How golden sets are structured\n",
        "2. How to run evaluations\n",
        "3. How to interpret results\n",
        "4. How to add new test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"../setup_agent\")\n",
        "\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "from evaluator import run_test_case, load_golden_set, check_tools, check_must_contain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding Golden Set Structure\n",
        "\n",
        "Let's look at how a golden set test case is defined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the golden set\n",
        "test_cases = load_golden_set()\n",
        "print(f\"Loaded {len(test_cases)} test cases\\n\")\n",
        "\n",
        "# Look at the first test case\n",
        "first_case = test_cases[0]\n",
        "print(\"Example test case:\")\n",
        "print(\"-\" * 40)\n",
        "for key, value in first_case.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Running a Single Test Case\n",
        "\n",
        "Let's run one test case and see what happens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the first test case\n",
        "result = run_test_case(test_cases[0], verbose=True)\n",
        "\n",
        "print(f\"Test ID: {result.id}\")\n",
        "print(f\"Query: {result.query}\")\n",
        "print(f\"Passed: {result.passed}\")\n",
        "print(f\"Tools Used: {result.tools_used}\")\n",
        "print(f\"\\nChecks:\")\n",
        "print(f\"  - Tool Check: {result.tool_check}\")\n",
        "print(f\"  - Source Check: {result.source_check}\")\n",
        "print(f\"  - Content Check: {result.content_check}\")\n",
        "print(f\"  - Negative Check: {result.negative_check}\")\n",
        "\n",
        "if result.errors:\n",
        "    print(f\"\\nErrors: {result.errors}\")\n",
        "\n",
        "print(f\"\\nResponse (first 500 chars):\")\n",
        "print(\"-\" * 40)\n",
        "print(result.response[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Understanding the Checks\n",
        "\n",
        "Each test case can have multiple checks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool check example\n",
        "expected_tools = [\"vector_search\"]\n",
        "actual_tools = [\"vector_search\"]\n",
        "\n",
        "passed, error = check_tools(expected_tools, actual_tools)\n",
        "print(f\"Tool Check: {'PASS' if passed else 'FAIL'}\")\n",
        "print(f\"Expected: {expected_tools}\")\n",
        "print(f\"Actual: {actual_tools}\")\n",
        "\n",
        "# Try with wrong tools\n",
        "actual_wrong = [\"sql_query\"]\n",
        "passed, error = check_tools(expected_tools, actual_wrong)\n",
        "print(f\"\\nWrong Tool Check: {'PASS' if passed else 'FAIL'}\")\n",
        "print(f\"Error: {error}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Content check example\n",
        "keywords = [\"remote\", \"core hours\", \"500\"]\n",
        "response = \"Our remote work policy includes core hours from 10 AM to 3 PM and a $500 stipend.\"\n",
        "\n",
        "passed, error = check_must_contain(keywords, response)\n",
        "print(f\"Content Check: {'PASS' if passed else 'FAIL'}\")\n",
        "print(f\"Looking for: {keywords}\")\n",
        "print(f\"In response: {response[:100]}...\")\n",
        "\n",
        "# Try with missing keyword\n",
        "response_missing = \"Our remote work policy includes core hours.\"\n",
        "passed, error = check_must_contain(keywords, response_missing)\n",
        "print(f\"\\nMissing Keyword Check: {'PASS' if passed else 'FAIL'}\")\n",
        "print(f\"Error: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Running Multiple Test Cases\n",
        "\n",
        "Let's run a few test cases to see the variety:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run a subset of tests (first 5)\n",
        "results = []\n",
        "for tc in test_cases[:5]:\n",
        "    print(f\"Running {tc['id']}: {tc['query'][:40]}...\")\n",
        "    result = run_test_case(tc)\n",
        "    results.append(result)\n",
        "    status = \"✓ PASS\" if result.passed else \"✗ FAIL\"\n",
        "    print(f\"  {status}\")\n",
        "    if not result.passed:\n",
        "        for err in result.errors:\n",
        "            print(f\"    - {err}\")\n",
        "    print()\n",
        "\n",
        "# Summary\n",
        "passed = sum(1 for r in results if r.passed)\n",
        "print(f\"Results: {passed}/{len(results)} passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Adding a New Test Case\n",
        "\n",
        "Here's how to add a new golden set test case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a new test case\n",
        "new_test = {\n",
        "    \"id\": \"gs-custom-001\",\n",
        "    \"category\": \"vector_search\",\n",
        "    \"query\": \"What are the code review requirements?\",\n",
        "    \"expected_tools\": [\"vector_search\"],\n",
        "    \"expected_sources\": [\"code_review_standards.md\"],\n",
        "    \"must_contain\": [\"review\", \"approval\"],\n",
        "    \"must_not_contain\": [\"I don't know\"],\n",
        "}\n",
        "\n",
        "# Run it\n",
        "print(\"Testing new case:\")\n",
        "print(f\"Query: {new_test['query']}\")\n",
        "print()\n",
        "\n",
        "result = run_test_case(new_test)\n",
        "print(f\"Passed: {result.passed}\")\n",
        "print(f\"Tools used: {result.tools_used}\")\n",
        "if result.errors:\n",
        "    print(f\"Errors: {result.errors}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you understand golden sets:\n",
        "\n",
        "1. Run the full suite: `uv run python evaluator.py`\n",
        "2. Add test cases for edge cases you discover\n",
        "3. Move on to **Stage 2: Labeled Scenarios** for broader coverage\n",
        "\n",
        "```bash\n",
        "cd ../stage_2_labeled_scenarios\n",
        "cat README.md\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
