{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 3: Replay Harnesses â€” Reproducibility & Rich Metrics\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ What You'll Learn\n",
        "\n",
        "By the end of this walkthrough, you'll understand:\n",
        "\n",
        "1. **What replay harnesses are** and why reproducibility matters\n",
        "2. **How recording and replay works** behind the scenes\n",
        "3. **The evaluation metrics** used in ML/AI â€” precision, recall, groundedness, faithfulness\n",
        "4. **How to use LLM-as-judge** for generation quality assessment\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“š Understanding Replay Harnesses\n",
        "\n",
        "### The Problem with Live Evaluation\n",
        "\n",
        "In Stages 1 and 2, every test run calls the actual agent:\n",
        "- LLM responses vary (even with temperature=0)\n",
        "- External services may behave differently\n",
        "- Tests aren't fully reproducible\n",
        "- Each run costs money\n",
        "\n",
        "**Replay harnesses solve this** by recording sessions and replaying them deterministically.\n",
        "\n",
        "### What Are Replay Harnesses?\n",
        "\n",
        "A replay harness has three components:\n",
        "\n",
        "1. **Recorder** â€” Captures agent sessions (query, tool calls, responses)\n",
        "2. **Player** â€” Replays sessions with cached responses (no LLM calls)\n",
        "3. **Evaluator** â€” Computes rich ML metrics on replayed sessions\n",
        "\n",
        "| Stage 1 & 2 | Stage 3: Replay |\n",
        "|-------------|-----------------|\n",
        "| Live agent calls | Cached responses |\n",
        "| Variable results | Deterministic |\n",
        "| Simple pass/fail | Rich metrics |\n",
        "| $ per run | Free replay |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—ï¸ Where Replay Harnesses Fit: The Eval Maturity Model\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    EVAL FRAMEWORK MATURITY                      â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  Stage 5: Experiments      â† Compare configurations            â”‚\n",
        "â”‚      â–²                                                          â”‚\n",
        "â”‚  Stage 4: Rubrics          â† Multi-dimensional scoring         â”‚\n",
        "â”‚      â–²                                                          â”‚\n",
        "â”‚  â˜… Stage 3: REPLAY HARNESSES â˜… â† Reproducibility (YOU ARE HERE)â”‚\n",
        "â”‚      â–²                                                          â”‚\n",
        "â”‚  Stage 2: Labeled Scenariosâ† Coverage mapping (completed)      â”‚\n",
        "â”‚      â–²                                                          â”‚\n",
        "â”‚  Stage 1: Golden Sets      â† Baseline correctness (completed)  â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "Replay harnesses add **reproducibility** and **proper ML metrics** to your evaluation framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ” Behind the Scenes: The Record/Replay Cycle\n",
        "\n",
        "Here's how the replay harness works:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    RECORD/REPLAY CYCLE                                â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                       â”‚\n",
        "â”‚  PHASE 1: RECORD (one-time, costs $)                                 â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
        "â”‚  â”‚                                                             â”‚     â”‚\n",
        "â”‚  â”‚  Query â”€â”€â–º Agent â”€â”€â–º Tool Calls â”€â”€â–º LLM Response            â”‚     â”‚\n",
        "â”‚  â”‚                          â”‚              â”‚                   â”‚     â”‚\n",
        "â”‚  â”‚                          â–¼              â–¼                   â”‚     â”‚\n",
        "â”‚  â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚     â”‚\n",
        "â”‚  â”‚                    â”‚   Session JSON       â”‚                 â”‚     â”‚\n",
        "â”‚  â”‚                    â”‚   - query            â”‚                 â”‚     â”‚\n",
        "â”‚  â”‚                    â”‚   - tool_calls       â”‚                 â”‚     â”‚\n",
        "â”‚  â”‚                    â”‚   - response         â”‚                 â”‚     â”‚\n",
        "â”‚  â”‚                    â”‚   - sources          â”‚                 â”‚     â”‚\n",
        "â”‚  â”‚                    â”‚   - annotations      â”‚                 â”‚     â”‚\n",
        "â”‚  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚     â”‚\n",
        "â”‚  â”‚                              â”‚                              â”‚     â”‚\n",
        "â”‚  â”‚                              â–¼                              â”‚     â”‚\n",
        "â”‚  â”‚                    fixtures/session-001.json                â”‚     â”‚\n",
        "â”‚  â”‚                                                             â”‚     â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
        "â”‚                                                                       â”‚\n",
        "â”‚  PHASE 2: ANNOTATE (human review)                                    â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
        "â”‚  â”‚  Add ground truth:                                          â”‚     â”‚\n",
        "â”‚  â”‚   â€¢ relevant_sources: [\"refund_policy.md\"]                  â”‚     â”‚\n",
        "â”‚  â”‚   â€¢ expected_facts: [\"30-day window\", \"8 refunds\"]          â”‚     â”‚\n",
        "â”‚  â”‚   â€¢ expected_tools: [\"vector_search\", \"sql_query\"]          â”‚     â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
        "â”‚                                                                       â”‚\n",
        "â”‚  PHASE 3: REPLAY (unlimited, free)                                   â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
        "â”‚  â”‚                                                             â”‚     â”‚\n",
        "â”‚  â”‚  Load Session â”€â”€â–º Compare Against Annotations               â”‚     â”‚\n",
        "â”‚  â”‚                              â”‚                              â”‚     â”‚\n",
        "â”‚  â”‚                              â–¼                              â”‚     â”‚\n",
        "â”‚  â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚     â”‚\n",
        "â”‚  â”‚                    â”‚   Compute Metrics    â”‚                 â”‚     â”‚\n",
        "â”‚  â”‚                    â”‚   - Precision: 1.0   â”‚                 â”‚     â”‚\n",
        "â”‚  â”‚                    â”‚   - Recall: 1.0      â”‚                 â”‚     â”‚\n",
        "â”‚  â”‚                    â”‚   - Groundedness: 95%â”‚                 â”‚     â”‚\n",
        "â”‚  â”‚                    â”‚   - Faithfulness: 100â”‚                 â”‚     â”‚\n",
        "â”‚  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚     â”‚\n",
        "â”‚  â”‚                                                             â”‚     â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
        "â”‚                                                                       â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "**Key insight:** You record once, then replay and evaluate unlimited times for free."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š The Evaluation Metrics\n",
        "\n",
        "Replay harnesses use **proper ML/AI metrics** instead of just pass/fail. Let's understand each category:\n",
        "\n",
        "### Retrieval Metrics (RAG Quality)\n",
        "\n",
        "These measure how well your agent finds relevant documents:\n",
        "\n",
        "| Metric | Formula | What It Tells You |\n",
        "|--------|---------|-------------------|\n",
        "| **Precision** | relevant_retrieved / total_retrieved | \"Of what we found, how much was useful?\" |\n",
        "| **Recall** | relevant_retrieved / total_relevant | \"Of what exists, how much did we find?\" |\n",
        "| **F1 Score** | 2 Ã— (P Ã— R) / (P + R) | Harmonic mean of precision and recall |\n",
        "| **MRR** | 1 / rank_of_first_relevant | \"How quickly do we find something good?\" |\n",
        "\n",
        "**Example:**\n",
        "- Ground truth relevant docs: `[doc_A, doc_B]`\n",
        "- Agent retrieved: `[doc_A, doc_C]`\n",
        "- Precision = 1/2 = 50% (only doc_A was relevant)\n",
        "- Recall = 1/2 = 50% (missed doc_B)\n",
        "\n",
        "### Generation Metrics (LLM-as-Judge)\n",
        "\n",
        "These measure response quality using another LLM as an evaluator:\n",
        "\n",
        "| Metric | What It Measures | How It Works |\n",
        "|--------|-----------------|--------------|\n",
        "| **Groundedness** | Claims supported by sources | Judge checks each claim against retrieved docs |\n",
        "| **Faithfulness** | No hallucinated facts | Judge looks for made-up information |\n",
        "| **Relevance** | Answers the question (1-5) | Judge rates if response addresses the query |\n",
        "| **Completeness** | Covers all aspects | Judge checks if all parts are addressed |\n",
        "\n",
        "### Tool Metrics\n",
        "\n",
        "| Metric | What It Measures |\n",
        "|--------|-----------------|\n",
        "| **Tool Accuracy** | Did it use the correct tools? |\n",
        "| **Tool Efficiency** | Did it avoid unnecessary calls? |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Import required modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd().parent / \"setup_agent\"))\n",
        "\n",
        "from recorder import Session, load_session, list_sessions, record_session\n",
        "from player import replay_session, get_session_summary\n",
        "from metrics import precision, recall, f1_score, mrr, groundedness, faithfulness, relevance\n",
        "from evaluator import evaluate_session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’» Hands-On: Working with Sessions\n",
        "\n",
        "Let's explore the recorded sessions (fixtures) and see how replay works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List existing recorded sessions\n",
        "sessions = list_sessions()\n",
        "print(f\"ðŸ“ Found {len(sessions)} recorded sessions:\\n\")\n",
        "\n",
        "for session_id in sessions:\n",
        "    try:\n",
        "        summary = get_session_summary(session_id)\n",
        "        print(f\"  ðŸ“ {session_id}\")\n",
        "        print(f\"     Query: {summary['query'][:50]}...\")\n",
        "        print(f\"     Tools: {summary['tool_calls']}\")\n",
        "        print(f\"     Has annotations: {summary['has_annotations']}\")\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸ {session_id}: Could not load - {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§® Understanding Retrieval Metrics\n",
        "\n",
        "Let's compute precision and recall manually to understand what they mean:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Computing retrieval metrics\n",
        "print(\"ðŸ§® RETRIEVAL METRICS EXAMPLE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Scenario: Agent needs to find refund-related docs\n",
        "relevant_docs = [\"refund_policy.md\", \"pricing_strategy.md\"]  # Ground truth\n",
        "retrieved_docs = [\"refund_policy.md\", \"pto_policy.md\"]        # What agent found\n",
        "\n",
        "print(f\"\\nGround truth relevant: {relevant_docs}\")\n",
        "print(f\"Agent retrieved:       {retrieved_docs}\")\n",
        "\n",
        "# Calculate metrics\n",
        "p = precision(retrieved_docs, relevant_docs)\n",
        "r = recall(retrieved_docs, relevant_docs)\n",
        "f1 = f1_score(retrieved_docs, relevant_docs)\n",
        "mrr_score = mrr(retrieved_docs, relevant_docs)\n",
        "\n",
        "print(f\"\\nðŸ“Š Results:\")\n",
        "print(f\"   Precision: {p:.2f} â†’ Of 2 retrieved, 1 was relevant (50%)\")\n",
        "print(f\"   Recall:    {r:.2f} â†’ Of 2 relevant, 1 was found (50%)\")\n",
        "print(f\"   F1 Score:  {f1:.2f} â†’ Harmonic mean\")\n",
        "print(f\"   MRR:       {mrr_score:.2f} â†’ First relevant at position 1\")\n",
        "\n",
        "# Perfect retrieval example\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Perfect retrieval scenario:\")\n",
        "retrieved_perfect = [\"refund_policy.md\", \"pricing_strategy.md\"]\n",
        "print(f\"Retrieved: {retrieved_perfect}\")\n",
        "print(f\"Precision: {precision(retrieved_perfect, relevant_docs):.2f}\")\n",
        "print(f\"Recall:    {recall(retrieved_perfect, relevant_docs):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ¤– LLM-as-Judge: Generation Metrics\n",
        "\n",
        "For generation quality, we use another LLM to evaluate the response. This is called \"LLM-as-judge\" and is the standard approach for evaluating language quality.\n",
        "\n",
        "### How Groundedness Works\n",
        "\n",
        "The judge receives the sources and response, then checks each claim:\n",
        "\n",
        "```\n",
        "Sources: \"Refund policy: 30-day guarantee on annual plans...\"\n",
        "Response: \"We offer a 30-day guarantee for annual plans.\"\n",
        "\n",
        "Claim 1: \"30-day guarantee\" â†’ GROUNDED (in source)\n",
        "Claim 2: \"annual plans\" â†’ GROUNDED (in source)\n",
        "\n",
        "Score: 2/2 = 100% grounded\n",
        "```\n",
        "\n",
        "### How Faithfulness Differs\n",
        "\n",
        "Faithfulness specifically checks for **hallucinations** â€” facts that are fabricated and NOT in sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Evaluating response groundedness\n",
        "print(\"ðŸ¤– LLM-AS-JUDGE EXAMPLE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Simulated response and sources\n",
        "sources = [\n",
        "    \"Refund Policy: We offer a 30-day money-back guarantee on all annual plans. \"\n",
        "    \"Customers must submit refund requests through our support portal.\"\n",
        "]\n",
        "\n",
        "# Good response (grounded)\n",
        "good_response = \"Our refund policy provides a 30-day money-back guarantee for annual plans. To request a refund, contact support.\"\n",
        "\n",
        "# Bad response (with hallucination)\n",
        "bad_response = \"Our refund policy provides a 30-day money-back guarantee for annual plans. We also offer a 60-day guarantee for enterprise customers.\"\n",
        "\n",
        "print(\"Sources:\", sources[0][:80] + \"...\")\n",
        "print()\n",
        "print(\"Good response (should be grounded):\")\n",
        "print(f\"  '{good_response}'\")\n",
        "print()\n",
        "print(\"Bad response (contains hallucination):\")\n",
        "print(f\"  '{bad_response}'\")\n",
        "print(\"  ^ '60-day guarantee for enterprise' is NOT in the source!\")\n",
        "\n",
        "# Note: Actual groundedness check requires API call\n",
        "print(\"\\nâš ï¸ Note: Full groundedness evaluation requires an LLM call (costs $)\")\n",
        "print(\"   Run evaluator.py to see real scores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”´ Recording a New Session\n",
        "\n",
        "Let's record a new session to see the full flow. This will:\n",
        "1. Send a query to the actual agent (costs $)\n",
        "2. Capture all tool calls and the response\n",
        "3. Save it to a JSON fixture file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Record a new session (this calls the actual agent - costs $)\n",
        "# Uncomment to run:\n",
        "\n",
        "# session = record_session(\n",
        "#     query=\"What is the home office equipment stipend?\",\n",
        "#     session_id=\"demo_session\"\n",
        "# )\n",
        "# \n",
        "# print(f\"\\\\nâœ… Session recorded!\")\n",
        "# print(f\"   ID: {session.session_id}\")\n",
        "# print(f\"   Tools used: {[tc.tool_name for tc in session.tool_calls]}\")\n",
        "# print(f\"   Sources found: {session.retrieved_sources}\")\n",
        "\n",
        "print(\"ðŸ’¡ To record a session, uncomment the code above and run the cell.\")\n",
        "print(\"   This will call the actual agent (costs $).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## â–¶ï¸ Replaying a Session\n",
        "\n",
        "Once recorded, you can replay sessions unlimited times without API calls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replay an existing session\n",
        "sessions = list_sessions()\n",
        "\n",
        "if sessions:\n",
        "    session_id = sessions[0]  # Use first available session\n",
        "    print(f\"â–¶ï¸ Replaying session: {session_id}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    session = replay_session(session_id)\n",
        "    \n",
        "    print(f\"\\nðŸ“ Query: {session.query}\")\n",
        "    print(f\"ðŸ”§ Tools: {[tc.tool_name for tc in session.tool_calls]}\")\n",
        "    print(f\"ðŸ“š Sources: {session.retrieved_sources}\")\n",
        "    print(f\"\\nðŸ“„ Response preview:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(session.response[:400] + \"...\" if len(session.response) > 400 else session.response)\n",
        "else:\n",
        "    print(\"âš ï¸ No sessions found. Record one first using record_session()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Full Session Evaluation\n",
        "\n",
        "The evaluator runs all metrics on a session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run full evaluation on a session\n",
        "sessions = list_sessions()\n",
        "\n",
        "if sessions:\n",
        "    session_id = sessions[0]\n",
        "    session = load_session(session_id)\n",
        "    \n",
        "    print(f\"ðŸ“Š Evaluating session: {session_id}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # This will call LLM judges for generation metrics\n",
        "    results = evaluate_session(session, verbose=True)\n",
        "    \n",
        "    print(f\"\\nðŸ† Overall Score: {results['overall']:.2f}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No sessions to evaluate. Record one first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ“ Key Takeaways\n",
        "\n",
        "1. **Record once, replay forever** â€” Deterministic evaluation without API costs\n",
        "2. **Proper ML metrics** â€” Precision, recall, F1 for retrieval quality\n",
        "3. **LLM-as-judge** â€” Groundedness and faithfulness for generation quality\n",
        "4. **Annotations are key** â€” Ground truth enables meaningful metrics\n",
        "5. **Version your fixtures** â€” Commit session JSONs to git for reproducibility\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ˆ Metrics Quick Reference\n",
        "\n",
        "| Metric | Good Score | Warning Signs |\n",
        "|--------|------------|---------------|\n",
        "| Precision | > 0.8 | Retrieving irrelevant docs |\n",
        "| Recall | > 0.8 | Missing relevant docs |\n",
        "| Groundedness | > 0.9 | Claims not supported by sources |\n",
        "| Faithfulness | 1.0 | Any hallucination is a problem |\n",
        "| Relevance | 4-5/5 | Response doesn't address query |\n",
        "\n",
        "---\n",
        "\n",
        "## â­ï¸ What's Next?\n",
        "\n",
        "Replay harnesses give you rich metrics, but they're still pass/fail in nature. What about **gradual quality degradation**?\n",
        "\n",
        "- A response can be technically correct but poorly written\n",
        "- Some dimensions matter more than others (accuracy > clarity for compliance)\n",
        "- You want to track quality trends over time\n",
        "\n",
        "**Stage 4: Rubric-Based Evaluation** addresses this with multi-dimensional scoring:\n",
        "\n",
        "```bash\n",
        "cd ../stage_4_rubrics\n",
        "uv run jupyter notebook walkthrough.ipynb\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
