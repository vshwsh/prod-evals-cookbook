{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 5: Experiments ‚Äî Data-Driven Agent Optimization\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ What You'll Learn\n",
        "\n",
        "By the end of this walkthrough, you'll understand:\n",
        "\n",
        "1. **What experiments are** and why gut feelings aren't enough\n",
        "2. **How to define variants** (different configurations to compare)\n",
        "3. **How to run controlled experiments** with consistent test sets\n",
        "4. **How to analyze results** and make data-driven decisions\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Understanding Experiments\n",
        "\n",
        "### The Problem: Too Many Knobs\n",
        "\n",
        "When improving your agent, you face questions like:\n",
        "- \"Is GPT-4o worth the extra cost vs GPT-4o-mini?\"\n",
        "- \"Does this new system prompt reduce hallucinations?\"\n",
        "- \"What temperature gives the best quality/latency tradeoff?\"\n",
        "\n",
        "**You could guess.** Or you could **run experiments and know for sure.**\n",
        "\n",
        "### What Are Experiments?\n",
        "\n",
        "Experiments run the **same test suite across different agent configurations**, collecting:\n",
        "- Pass rates\n",
        "- Quality scores (rubrics)\n",
        "- Latency\n",
        "- Cost\n",
        "- Tool usage patterns\n",
        "\n",
        "Then you compare the results and pick the winner.\n",
        "\n",
        "| Intuition-Based | Experiment-Based |\n",
        "|-----------------|------------------|\n",
        "| \"I think GPT-4o is better\" | \"GPT-4o scores 4.5/5 vs 4.1/5\" |\n",
        "| \"The new prompt seems good\" | \"New prompt: 95% pass vs 87%\" |\n",
        "| \"Faster is always better\" | \"50ms faster but 5% less accurate\" |\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è Where Experiments Fit: The Eval Maturity Model\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    EVAL FRAMEWORK MATURITY                      ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                 ‚îÇ\n",
        "‚îÇ  ‚òÖ Stage 5: EXPERIMENTS ‚òÖ  ‚Üê Compare configurations (YOU ARE HERE)‚îÇ\n",
        "‚îÇ      ‚ñ≤                                                          ‚îÇ\n",
        "‚îÇ  Stage 4: Rubrics          ‚Üê Multi-dimensional scoring (done)  ‚îÇ\n",
        "‚îÇ      ‚ñ≤                                                          ‚îÇ\n",
        "‚îÇ  Stage 3: Replay Harnesses ‚Üê Reproducibility (done)            ‚îÇ\n",
        "‚îÇ      ‚ñ≤                                                          ‚îÇ\n",
        "‚îÇ  Stage 2: Labeled Scenarios‚Üê Coverage mapping (done)           ‚îÇ\n",
        "‚îÇ      ‚ñ≤                                                          ‚îÇ\n",
        "‚îÇ  Stage 1: Golden Sets      ‚Üê Baseline correctness (done)       ‚îÇ\n",
        "‚îÇ                                                                 ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**This is the top of the maturity model.** Experiments enable continuous improvement through data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Behind the Scenes: How Experiments Work\n",
        "\n",
        "Here's the experiment workflow:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    EXPERIMENT WORKFLOW                                ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                       ‚îÇ\n",
        "‚îÇ  STEP 1: DEFINE VARIANTS                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ  ‚îÇ variants.yaml:                                               ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ                                                              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   baseline:                                                  ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ     model: gpt-4o-mini                                       ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ     temperature: 0.1                                         ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ     system_prompt: v1                                        ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ                                                              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   gpt4o_upgrade:                                             ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ     model: gpt-4o         ‚Üê Change one thing                 ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ     temperature: 0.1                                         ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ     system_prompt: v1                                        ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ                                                              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   new_prompt:                                                ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ     model: gpt-4o-mini                                       ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ     temperature: 0.1                                         ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ     system_prompt: v2     ‚Üê Change one thing                 ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îÇ                            ‚îÇ                                          ‚îÇ\n",
        "‚îÇ                            ‚ñº                                          ‚îÇ\n",
        "‚îÇ  STEP 2: RUN TEST SUITE FOR EACH VARIANT                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ  ‚îÇ For each variant:                                            ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   1. Create agent with variant config                        ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   2. Run all golden set test cases                           ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   3. Collect: pass/fail, rubric scores, latency, tokens      ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   4. Save detailed results to JSON                           ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îÇ                            ‚îÇ                                          ‚îÇ\n",
        "‚îÇ                            ‚ñº                                          ‚îÇ\n",
        "‚îÇ  STEP 3: COMPARE RESULTS                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ  ‚îÇ                                                              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ  Variant        Pass %   Rubric   Latency    Cost            ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ           ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ  baseline         87%     4.1/5    1.2s     $0.003           ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ  gpt4o_upgrade    93%     4.5/5    2.1s     $0.015  ‚Üê Best!  ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ  new_prompt       91%     4.3/5    1.3s     $0.003           ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ                                                              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îÇ                            ‚îÇ                                          ‚îÇ\n",
        "‚îÇ                            ‚ñº                                          ‚îÇ\n",
        "‚îÇ  STEP 4: DECIDE                                                      ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ  ‚îÇ  \"GPT-4o is 6% more accurate and 0.4 higher rubric score,   ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   but costs 5x more and is 75% slower.                       ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ                                                              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ   Decision: Use GPT-4o for high-stakes queries,              ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îÇ            GPT-4o-mini for everything else.\"                 ‚îÇ     ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îÇ                                                                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**Key principle:** Change one variable at a time to understand its impact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Import required modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd().parent / \"setup_agent\"))\n",
        "sys.path.insert(0, str(Path.cwd().parent / \"stage_4_rubrics\"))\n",
        "\n",
        "import yaml\n",
        "from runner import load_variants, load_test_cases, run_experiment, print_comparison\n",
        "from reporter import load_results, print_comparison_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíª Hands-On: Exploring Variant Definitions\n",
        "\n",
        "Let's look at how variants are defined in `variants.yaml`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display variant definitions\n",
        "variants_path = Path(\"variants.yaml\")\n",
        "\n",
        "with open(variants_path) as f:\n",
        "    variants_config = yaml.safe_load(f)\n",
        "\n",
        "print(\"üìã VARIANT DEFINITIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüîß Defaults (applied to all variants unless overridden):\")\n",
        "for key, value in variants_config.get(\"defaults\", {}).items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "print(\"\\nüìä Variants:\")\n",
        "for name, config in variants_config.get(\"variants\", {}).items():\n",
        "    print(f\"\\n   {name}:\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"      {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Understanding Variant Parameters\n",
        "\n",
        "Each variant can customize these parameters:\n",
        "\n",
        "| Parameter | Description | Example Values |\n",
        "|-----------|-------------|----------------|\n",
        "| `model` | LLM model to use | `gpt-4o`, `gpt-4o-mini`, `gpt-3.5-turbo` |\n",
        "| `temperature` | Response randomness | `0.0` (deterministic) to `1.0` (creative) |\n",
        "| `system_prompt` | Which prompt version | `v1`, `v2` (files in `prompts/`) |\n",
        "| `max_tokens` | Max response length | `500`, `1000`, `2000` |\n",
        "\n",
        "### Best Practice: One Change at a Time\n",
        "\n",
        "```yaml\n",
        "# ‚ùå Bad: Multiple changes - can't tell what helped\n",
        "experiment_v2:\n",
        "  model: gpt-4o        # Changed\n",
        "  temperature: 0.0     # Changed  \n",
        "  system_prompt: v2    # Changed\n",
        "\n",
        "# ‚úÖ Good: Single change - clear attribution\n",
        "gpt4o_test:\n",
        "  model: gpt-4o        # Changed\n",
        "  temperature: 0.1     # Same\n",
        "  system_prompt: v1    # Same\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Viewing Test Cases\n",
        "\n",
        "Experiments run on test cases from previous stages. Let's see what's available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test cases from different sources\n",
        "print(\"üìä TEST CASE SOURCES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for source in [\"golden\", \"scenarios\", \"rubrics\"]:\n",
        "    try:\n",
        "        cases = load_test_cases(source)\n",
        "        print(f\"\\n{source.upper()}: {len(cases)} test cases\")\n",
        "        # Show first 3\n",
        "        for case in cases[:3]:\n",
        "            query = case.get(\"query\", \"N/A\")[:50]\n",
        "            print(f\"   ‚Ä¢ {query}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{source.upper()}: Could not load - {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèÉ Running an Experiment\n",
        "\n",
        "Now let's run an experiment! This will:\n",
        "1. Create an agent for each variant\n",
        "2. Run all test cases\n",
        "3. Collect metrics\n",
        "4. Save results to disk\n",
        "\n",
        "**‚ö†Ô∏è Note:** This makes real API calls and costs money. Use `--limit` for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run a small experiment (3 test cases per variant to save costs)\n",
        "# Uncomment to run:\n",
        "\n",
        "# results = run_experiment(\n",
        "#     variant_names=[\"baseline\", \"new_prompt\"],  # Which variants to test\n",
        "#     test_source=\"golden\",                       # Use golden set test cases\n",
        "#     limit=3,                                    # Only 3 cases (for demo)\n",
        "#     include_rubrics=True,                       # Also score with rubrics\n",
        "#     verbose=True\n",
        "# )\n",
        "#\n",
        "# # Print comparison\n",
        "# print_comparison(results)\n",
        "\n",
        "print(\"üí° To run an experiment, uncomment the code above.\")\n",
        "print(\"   This will make real API calls (costs $).\")\n",
        "print()\n",
        "print(\"   Or run from command line:\")\n",
        "print(\"   uv run python runner.py --test-source golden --limit 3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Analyzing Results\n",
        "\n",
        "After running experiments, the reporter generates comparison tables. Here's how to interpret them:\n",
        "\n",
        "### The Comparison Table\n",
        "\n",
        "```\n",
        "Variant        Pass %   Rubric   Latency    Cost\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "baseline         87%     4.1/5    1.2s     $0.003\n",
        "gpt4o_upgrade    93%     4.5/5    2.1s     $0.015  ‚Üê Best quality\n",
        "new_prompt       91%     4.3/5    1.3s     $0.003  ‚Üê Best value\n",
        "```\n",
        "\n",
        "### What Each Column Means\n",
        "\n",
        "| Column | Meaning | Good Value |\n",
        "|--------|---------|------------|\n",
        "| **Pass %** | Percentage of test cases passing | Higher is better |\n",
        "| **Rubric** | Average quality score (1-5) | > 4.0 is good |\n",
        "| **Latency** | Average response time | Lower is better |\n",
        "| **Cost** | Estimated $ per query | Lower is better |\n",
        "\n",
        "### Making Decisions\n",
        "\n",
        "The \"best\" variant depends on your priorities:\n",
        "\n",
        "| Priority | Choose | Why |\n",
        "|----------|--------|-----|\n",
        "| **Quality first** | Highest rubric score | Accept higher cost/latency |\n",
        "| **Cost first** | Lowest cost with acceptable quality | gpt-4o-mini usually |\n",
        "| **Balanced** | Best quality/cost ratio | Often new prompts help |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Viewing Past Experiment Results\n",
        "\n",
        "Results are saved to the `results/` directory. Let's check for existing experiments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for existing experiment results\n",
        "results_dir = Path(\"results\")\n",
        "\n",
        "if results_dir.exists():\n",
        "    result_files = list(results_dir.glob(\"*.json\"))\n",
        "    print(f\"üìÅ Found {len(result_files)} result files in results/\")\n",
        "    \n",
        "    for f in result_files[:5]:  # Show first 5\n",
        "        print(f\"   ‚Ä¢ {f.name}\")\n",
        "    \n",
        "    if result_files:\n",
        "        print(\"\\\\nüìä Loading and displaying results...\")\n",
        "        try:\n",
        "            summaries = load_results(str(results_dir))\n",
        "            if summaries:\n",
        "                print_comparison_table(summaries)\n",
        "            else:\n",
        "                print(\"   No valid summaries found in results.\")\n",
        "        except Exception as e:\n",
        "            print(f\"   Could not load results: {e}\")\n",
        "else:\n",
        "    print(\"üìÅ No results directory found.\")\n",
        "    print(\"   Run an experiment first: uv run python runner.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Common Experiments to Run\n",
        "\n",
        "Here are experiments you should run for your agent:\n",
        "\n",
        "### 1. Model Comparison\n",
        "```yaml\n",
        "variants:\n",
        "  mini: { model: gpt-4o-mini }\n",
        "  full: { model: gpt-4o }\n",
        "```\n",
        "**Question:** Is the bigger model worth 10x the cost?\n",
        "\n",
        "### 2. Temperature Tuning\n",
        "```yaml\n",
        "variants:\n",
        "  deterministic: { temperature: 0.0 }\n",
        "  balanced: { temperature: 0.3 }\n",
        "  creative: { temperature: 0.7 }\n",
        "```\n",
        "**Question:** How much randomness is optimal?\n",
        "\n",
        "### 3. Prompt Engineering\n",
        "```yaml\n",
        "variants:\n",
        "  current: { system_prompt: v1 }\n",
        "  detailed: { system_prompt: v2 }\n",
        "  concise: { system_prompt: v3 }\n",
        "```\n",
        "**Question:** Does prompt wording affect quality?\n",
        "\n",
        "### 4. Tool Selection\n",
        "```yaml\n",
        "variants:\n",
        "  all_tools: { tools: [\"vector\", \"sql\", \"jira\", \"slack\"] }\n",
        "  limited: { tools: [\"vector\", \"sql\"] }\n",
        "```\n",
        "**Question:** Do more tools help or hurt?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Key Takeaways\n",
        "\n",
        "1. **Data beats intuition** ‚Äî Run experiments instead of guessing\n",
        "2. **One change at a time** ‚Äî Isolate variables to understand impact\n",
        "3. **Same test set** ‚Äî Compare apples to apples across variants\n",
        "4. **Track all metrics** ‚Äî Quality, latency, and cost together\n",
        "5. **Document decisions** ‚Äî Record why you chose a configuration\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Experiment Checklist\n",
        "\n",
        "Before running an experiment:\n",
        "- [ ] Define clear hypothesis (\"New prompt will improve accuracy\")\n",
        "- [ ] Create variant with single change\n",
        "- [ ] Choose appropriate test set (golden for quick, scenarios for thorough)\n",
        "- [ ] Set reasonable sample size (more = more confidence but more $)\n",
        "\n",
        "After running:\n",
        "- [ ] Compare all metrics, not just one\n",
        "- [ ] Consider cost/quality tradeoffs\n",
        "- [ ] Document the decision and rationale\n",
        "- [ ] Update production config if winner is found\n",
        "- [ ] Set up regression tests to protect the improvement\n",
        "\n",
        "---\n",
        "\n",
        "## üèÜ Congratulations!\n",
        "\n",
        "You've completed all five stages of the Production Evals Cookbook:\n",
        "\n",
        "| Stage | What You Learned |\n",
        "|-------|-----------------|\n",
        "| 1. Golden Sets | Baseline correctness with curated test cases |\n",
        "| 2. Labeled Scenarios | Coverage mapping with categorized tests |\n",
        "| 3. Replay Harnesses | Reproducibility with recorded sessions |\n",
        "| 4. Rubrics | Quality scoring with multi-dimensional rubrics |\n",
        "| 5. Experiments | Configuration optimization with controlled experiments |\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "1. **Integrate into CI** ‚Äî Run golden sets on every commit\n",
        "2. **Set up monitoring** ‚Äî Track rubric scores over time\n",
        "3. **Build feedback loops** ‚Äî Turn production issues into test cases\n",
        "4. **Iterate** ‚Äî Use experiments to continuously improve\n",
        "\n",
        "Happy evaluating! üöÄ"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
